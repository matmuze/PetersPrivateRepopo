\begin{figure}[t]
 \centering
 \subfloat[]{\label{fig:df0}\includegraphics[width=0.25\linewidth]{figures/df0.eps}}
 \subfloat[]{\label{fig:df1}\includegraphics[width=0.25\linewidth]{figures/df1.eps}}
 \subfloat[]{\label{fig:df2}\includegraphics[width=0.25\linewidth]{figures/df2.eps}} 
 \subfloat[]{\label{fig:df2}\includegraphics[width=0.25\linewidth]{figures/df3.eps}}
 \caption{\label{fig:df}View-space clipping, (a) shows the full HIV Capsid, (b) shows the uniformly distributed clipping, (c) demonstrate the aperture effect and (d) shows the results of the 2D distance transform of the clipping mask.}
\end{figure}

\begin{figure}[t]
 \centering
 \subfloat[]{\label{fig:df0}\includegraphics[width=0.495\linewidth]{figures/islands-no.eps}}
 \subfloat[]{\label{fig:islands1}\includegraphics[width=0.495\linewidth]{figures/islands-yes.eps}}
 \caption{\label{fig:islands} The occlusion clipping principle using a depth-stencil mask, the instances in red constitute the object of focus and the instances in grey are the potential occluders. The bar chart represents the depth-stencil of the mask. In (a) the occluders are overlapping the depth-stencil mask and will therefore be discarded. In figure (b) the instances (in blue) will be preserved after applying a depth bias on the overlapping red instance, allowing us to perform contextual anchoring described in Section \ref{ssec:anchoring}.}
\end{figure}

%\begin{figure}[t]
%\centering 
%\includegraphics[width=\linewidth]{figures/cutting-eq.eps}
%\caption{\label{fig:cutting-eq}Visual mapping between stack bar intervals %and instances.}
%\end{figure}

%SENTENCES DUMP

%Our data comprise of a dense set of macromolecules, encapsulated in compartments with several degrees of nesting. 
%Molecules are grouped by type and compartment, this information is contained in the scene file generated by cellPACK.
%Basic filtering parameters allows to manipulate the visibility of entire set of instances based on their type, independently or not from a geometrical cull object.
%Each cull object has its own parameters which are defined for all the ingredients type as shown in the overview Figure XX.
%When associated with geometrical shape, the cull object will only be influence to the region defined by the geometry, e.g, plane, sphere, cone...
%The cut objects how instances shall be discarded and they are applied sequentially.
%Internally the filtering is applied just after the object-space culling as shown in figure XX.
%User can modify these filtering parameters via the user interface.
%Additionally, there are more parameters which can influence when an instance is culled and which are related to a geometrical shape, those are defined in section XX.
%Additionally to defining where instances will be clipped, our system offer the mean to selectively chose the concentration of displayed elements for each protein types.
%These values are controlled by the user via the user interface. 

%Prior to the rendering each single instance is evaluated to determine if it shall be rendered.
%The cut objects how instances shall be discarded and they are applied sequentially.
%Internally the filtering is applied just after the object-space culling as shown in figure XX.
%First is applied the filtering based the clip probability.
%For each instance, we compare a uniformly distributed random number with the clip probability.
%If the random number is higher than the probability, the instance is marked as culled, and will not be rendered. 
%The random number is initially set for each individual instance and remain the same, in order to guaranty reproducibility of the scene.
%Secondly, instances are filtered according to their biochemical properties, for each cut object and each protein types the user defines ranges values for the both quantities and molecular weight.
%Instances whose properties lie outside on these ranges are marked as culled and discarded.
%For the book-keeping is the clipped ingredient we count for each ingredient type how many instances where discarded in total, for all active cut object.


\begin{comment}  
\section{Clipping}
Clipping objects define which instances of a given ingredient type are displayed.
There are two ways of how a clipping object can influence the clipping: either in object-space or in view-space.

\subsection{Object-Space Clipping}

\end{comment}  

\section{Object-Space Clipping}

Clipping objects define which instances of a given ingredient type are displayed.
There are two ways of how a clipping object can influence the clipping: either in object-space or in view-space.
%%%%%%%%%%%%%%%%%%%%%%
In this Section, we will explain in detail how clipping objects operate in object-space.
Using an object-space approach, the clipping objects will discard instances independent of the viewing direction. 
%Clipping objects are applied consecutively as explained in Figure \ref{fig:o}. This operation is recomputed every frame.
%The first step of the process is to determine, for each instance, if they are located in the clipping region.
%Once the instances are localized, they will be clipped according to object-space clipping parameters, which we describe in Section \ref{ssec:clip_params}.
%Moreover, we introduce the distance field falloff, an advanced parametrization for generating customizable gradient clipping effects.

\subsection{Clipping Region Localization}

Clipping objects can be associated with geometrical shapes to specify a sub-region of the domain which is influenced by the clipping.
Our system currently supports the following set of primitive shapes: plane, cube, sphere, cylinder and cone.
The first task of the object-space clipping is to determine whether an instance of a molecule is located in the sub-region defined by the clipping object geometry.
%This operation is repeated every frame for each instance of the scene.
To determine if an instance lies inside or outside the clipping object region, we compute the signed distance between the bounding sphere of the instance and the closest point on the surface of the clipping region . 
%Although supported shapes have a rather simple topology, it may still be computationally expensive, using a mesh-based representation, to compute a signed distance for a large number of instances.
%Indeed, using a triangle-based discretization would imply computing the signed distance between the instances and every single triangle of the mesh.
To accelerate the computation, we solve the problem analytically using a mathematical description of the 3D signed distance field (SDF).
%Using such representation instead reduces the problem of evaluating the signed distance to solving trivial equations.
It is also possible to apply simple SDF operators to the distance field, such as translation, rotation and scaling.
The clipping region can also be reversed by inverting the result of the signed distance function, offering users flexibility.
For instance, using a spherical shape, the clip region would be set to the inside of the sphere by default, while in inverted mode it would correspond to the outside of the sphere. 
Although the set of offered clipping shapes is yet limited, it would be trivial to enrich it using more complex SDF operators, such as union, to merge several shapes together in one single distance field, and thus obtain more complex clipping region shapes.

\subsection{Clipping Parameters} \label{ssec:clip_params}

A clipping object comprises two basic parameters, defined for each ingredient, that control the visibility of instances, based on their type.
%It is worth mentioning that in case no shape is associated with the cull object, the clipping will be evaluated for the entire domain.
The first parameter controls the ratio of clipped elements of a given type that are located in the clipping region.
We refer to this value as object-space clipping probability.
This parameter allows the user to control the degree of \textit{fuzziness} of the clipping, as explained in Figure \ref{fig:his}.
%While traditional cut-away methods support crisp cut, i.e., either shown of hidden, our technique allows for a more control over the strength of the cut as explained in Figure XX.
The other filtering parameters are related to biochemical properties and allow users to control the clipping based on the molecular mass or concentration of given ingredient types.
These parameters can be interactively changed via the user interface.
%Prior to the rendering, after determining the location of instances according to the clipping region, each instance is evaluated in order to determine if it shall be clipped.

First, filtering is applied based on the clipping probability.
For each instance, we compare a uniformly distributed random number with the clipping probability of the instance ingredient type.
If the random number is higher than the probability, the instance is marked as discarded, and will not be rendered. 
The random number is initially set for each individual instance and remains unchanged, in order to avoid getting different results each time.
Secondly, instances are filtered according to their biochemical properties, for each cut object and each ingredient type. The user defines a range of values for both quantities and the molecular weight.
Instances whose properties lie outside these ranges are marked as discarded and will not be rendered.

\subsection{Falloff Function}

To further increase the control over the probabilistic clipping, we introduce falloff functions. Falloff functions can be used to modulate the effect of the probabilistic clipping with respect to the distance from the clipping surface. This is easily achievable, since we use distance functions to represent the clipping objects. Therefore, the distance of a molecular instance from the clipping surface can be evaluated by simply sampling the distance function of the given clipping object at the 3D position of the instance. The farther away from the clipping surface the instance is, the lower will be the effect of the probabilistic clipping specified through the visibility equalizer.

%The falloff function can be either constant, linear, or exponential.

The object-space clipping probability of a molecule on the 3D position $p$ is multiplied by the falloff function $f(p)$. The falloff function is defined as follows:

\begin{equation}
	f(p) = 1 - min(1, (d(p) / d) ^ c)
\end{equation}

where $d(p)$ is the distance to the clipping surface from the point $p$. The function is parametrized by parameters $d$ and $c$, where $d$ is the maximum distance up to which the object-space clipping probability takes effect, while $c$ specifies the exponent of the falloff function.

%We provide additional parameters to gradually remove instances according the shape of the clipping region. These parameters control the so-called \emph{falloff functions}

An example of the falloff function applied to the object space clipping is shown in Figure \ref{fig:res:gh3}. The molecules of the blood serum (shown in red) are gradually removed from bottom to top. In this way, the information about the concentration of these molecules is illustrated at the bottom of the scene, while the visibility of the HIV particle (blue and green) is increased towards to top of the scene.

%\textbf{TODO PMINDEK:} Talk about gradient clipping here, motivation and parameters, maybe a figure too.


\section{View-Space Clipping}

While object-space clipping using primitive shapes allows for a great degree of flexibility, it requires cumbersome manual operations for complex set-ups, and is also limited in terms of shape diversity.
We therefore provide an additional functionality to manually specify a set of ingredient as object of focus, and to selectively remove occluding instances to ensure a maximum degree of visibility.
We use occlusion queries to determine what are the instances located in front of the focus region. 
We provide users the option to control the degree of fuzziness of the clipping, similarly to object-space clipping.
We also introduce a new effect inspired from the work of scientific illustrators, that allows users to control the degree of aperture of the view-space clipping.
Finally, we tweak our occlusion query method to improve depth perception by performing contextual anchoring of instances that should normally be clipped, when located in the close vicinity of the focus region.

\subsection{Occlusion Queries}

%Focus ingredients are priorly selected from the user interface.

%In order to clip instances that are occluding the focus region, the most optimum approach in terms of efficiency and ease of implementation is without a doubt image-based.

%Modern graphics hardware already a fixed function called occlusion queries (OQ) and which allow to determine whether an instance is visible or hidden according to previously drawn geometries.

%This approach, however, would require issuing one draw call per queries, which can seriously effect the frame rate when issuing hundred of thousands of queries, because of GPU driver overhead.

%The rendering tool we are using already allows to render the entire scene in a single call to avoid latency due to the GPU driver.

%Due to the potentially large number of instances in the scenes we showcase, we accelerate the computation of occluding instances using an image-based approach on the GPU.

We perform occlusion queries using an image-based approach on the GPU.
We priorly render an off-screen texture containing all the ingredients of the focus object, which we use as a stencil-depth mask to perform occlusion queries.
We only render the bounding spheres of the instances, in order to lower the cost of an additional render pass.
There can be several ingredient types constituting the object of focus, however, only one mask texture can be generated per clipping object.
To approach zero GPU driver overhead, we perform custom occlusion queries in a single draw call, using the programmable graphics pipeline. 
Subsequently, we draw the bounding sphere of all remaining instances over the mask.
Due to early depth-stencil test, implicitly performed by the graphics hardware, fragments of the bounding spheres that will pass the test are therefore are guaranteed to be overlapping fragments.
We then update the clipping state of the occluding instance by updating a flag stored in the main video memory directly from the fragment shader.
An example of how we perform occlusion queries is shown in Figure XX a.

\subsection{Clip Parameters}

We provide an additional parameter to control the degree of fuzziness of the view-dependant clipping, which we dub view-dependant clipping probability.
This value is priorly set by the user for each ingredient type.
Once we computed the occlusion state of an instance, we evaluate the clip probability, similarly to the way we evaluate object-space clipping. 
We compare the clipping probability with an uniformly distributed random number, initially set for each instance to determine if an occluding instance shall remain visible.
This will however result in a uniformly distributed number of visible occluders over the object of focus, which might not always be the best design choice because it fragments heavily the overall structure of the occluders compartment, see Figure XX.

We propose an alternative technique for fuzzy removal of occluding instances.
We define an additional parameter, called aperture coefficient, which controls the distance threshold in 2D from the centre to the edges of the mask, below which occluding instances shall be clipped.
A visual explanation of the aperture effect is shown in Figure X.
To enable this effect with compute the 2D distance transform of the occlusion mask which we store in a separate off-line texture.
We use the GPU Jump Flooding Algorithm by Rong \& Tan \cite{xxx} to interactively recompute the 2D distance field every frame. 
After computation of the distance transform, the texture stores for each pixel, the distance to the contours of the shape.
Then, while computing occlusion queries in the fragment shader, we may simply look-up the mask coutour distance for the current fragment, and discard instances accordingly to the user-defined aperture coefficient, priorly set of each ingredient type.

\subsection{Contextual Anchoring}
\label{ssec:anchoring}

When observing still renders of cut-away scenes, it might still be challenging to perceive the depth of objects correctly, despite using lighting-based depth cues.
We propose and additional method for depth guidance, which modifies the results of the clipping to preserve elements, located in a close range to non-clipped elements, and that would normally be clipped.
This way, the viewer, assuming that he is aware of what has been clipped, will intuitively understand where instances are located due to contextual anchoring.
This principle in shown in Figure XX, where we can observe parts of the green membrane anchored around channel molecules, and which indicate that the channel molecules are located on the surface of the object.
We were able to procedurally reproduce this effect by applying a depth bias for selected focused molecules to ensure that they will overlap ingredients that need to be preserved.
A explanation of the depth guidance effect is shown in Figure XX and a resulting render in show in Figure XX.

\section{Equalizing Visibility}

%The first section of the histogram (dark green region) shows the percentage of instances that are currently visible on the screen. 

%The entire green section (dark \& light green) represents the percentage of instances that are actually rendered.

To provide a clear overview of the scene properties, we display stack bars for each ingredient type that indicate information about their degree of visibility.
We chose to show three ranges for each histogram, respectively corresponding to the number of visible and clipped instances.
In order to fill the stack bars with correct values, we perform book-keeping of both clipped and visible instances, which we recompute every frame.
The principle of the book keeping is show in Figure \ref{fig:cutting-eq};
The stack bars are also interactive; the user can manipulate the clipping properties of the corresponding ingredient type by dragging the range handles, thus increasing or decreasing the number of displayed elements.

\subsection{Book-Keeping}

Modification of the visibility using the equalizer requires to know for each ingredient type, how many instances have been culled and how many instances are actually visible on the screen, see Figure xx.
It is worth mentioning that our system leverages the power of the GPU to compute the clipping state of each instance every frame.
Therefore, the information about clipping states is stored in the GPU memory.
In order to avoid overhead due to data transfer between CPU and GPU, we perform book-keeping on the GPU using atomic operations.
%Atomic operations are parallel programming features which guarantee mutual exclusion when simultaneous thread wish to write in the same memory location.
We priorly declare a buffer on the GPU to store the number of clipped and visible instances for each ingredient type.
To obtain the number of clipped instances, we simply increment the corresponding counter, each time an instance has been discarded using an atomic addition function.

For computing the number of visible instances, we first need information about actual visibility of rendered instances.
We render an additional off-screen texture where each pixel contains the internal ID of the instances.
We also declare an additional buffer to store a flag for each instance, which indicates if an instance is visible of hidden.
Then, by browsing through each pixel of the aforementioned ID texture in an additional pass, we update the visibility flag for the ID contained in each pixel.
Finally, we browse through each instance, and increment the corresponding counter accordingly to the visibility flag using an atomic addition function, similarly to the counting of clipped instances.

\subsection{User Interaction}

Upon manipulation of histogram ranges the system will either increase or decrease the number of clipped instances that correspond to the histogram ingredient type.
This is intended to optimise the way user interact with the system, by offering the user to directly manipulating quantities rather than abstract internal values such as the clip probability.
Additionally the user may also manipulate more advanced parameters in an additional UI panel.
We chose to map the motion of the histogram handles the clip probabilities.
The first handle manipulates the view-space clip probability, while the second handle controls the object-space one.
It is worth mentioning that the clip-probabilities that are manipulated by the user correspond to the currently selected clip-object.
The histogram view, however, remains the same when a different clip-object is selected.

Quantities are relative by default, i.e, they represent a percentage of the total number of instances for a given ingredient.
However, thanks to our design choices, they can also be displayed as absolute quantities with limited additional effort.
For displaying absolute quantities we support logarithmic scaling to ensure ingredients present in low quantities to be visible in the histograms.
An logarithmic ruler is also provided to help the user understanding the displayed values.

It is also worth mentioning that histograms are displayed in a tree layout where nodes correspond to compartments and leaves to ingredients.
Additional histograms are therefore displayed for each compartment by averaging the values of the ingredients contained inside.
Upon manipulation of compartment histograms properties, all the children ingredients will be updated accordingly.

%\section{Enhancements}

%Additionally to standard clipping functionality we developed a few enhancements to improve the way we perceive the scene.

%A crucial element when dealing with cluttered scenes is depth perception.

%Depth cues can be implemented by imitating the way light interact in reality to help us understanding distances between objects.

%However simulation illumination is rather expensive and prohibits a responsive user experience.

%We implement simplistic depth cues instead with a rough approximation of light behaviour.

%Additionally when provide option for context preserving view-space cut for a better understanding of the distance between objects in the view direction.

%Finally we perform rendering of the clipped element as ghosts in order to better communicate the overall proportions of visible elements in the scene.

%\subsection{Depth Cues}

%Depth cues are essential in molecular visualization and there exists many reference in the literature on how improve depth perception for that specific case.

%The first depth cue that we support is screen space ambient occlusion (SSAO) which allows to mimicate how global illumination works on the local scale, in image-space by evaluating for each pixel the depth of the surrounding pixels.

%Similarly to \cite{keylist}, we support two levels of ambient occlusion with different search radius to provide illumination for different levels of magnification.

%In addition to the limited range of the effect, SSAO does not account for directional light effects, which mean that shadows cannot be casted side-ways.

%When performing cut away however we perform strong shape alteration of the dataset which might be hard to perceive without shadows.

%Therefore, we additionally provide shadow mapping to better communicate the overall shape of the cuts.

%A downside of this approach is that it require an additional draw pass for each light that casts shadows onto molecules.

%Alternatively we also provide a cheaper method to provide additional cues about the shape of the clip objects.

%\textbf{TODO: PMINDEK, provide details about cheap depth cues}

%\subsection{Clipping Ghosts}

%While histograms allow us to visually monitor the quantities of removed instances, it might still be interesting to convey this information in the 3D scene while preserving  visibility settings.

%We additionally provide the option to render ghosts of all the clipped instances on top the scene, using alpha blending.

%We render all the ghosts in a separate off-screen texture which we later blend with the results of the scene rendering.

%The blending of the scene with the ghost texture is designed to ensure that the depth of the ghosts texture will merge with the scene's depth, and thus, that occluded ghosts will not be visible in the final results.

%We offer two rendering style for the ghosts, coloured filled or contour only.

%For each ingredient type we offer the option to render ghosts or not via the user interface.

%The resulting render of a scene with clipping ghosts can be seen in figure XX.
